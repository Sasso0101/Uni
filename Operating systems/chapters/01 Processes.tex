\chapter{Processes}

A process is a program in execution. Processes are identified by a \bold{process identifier} (pid). The OS allocates some space in RAM for the process in the following way:

\begin{itemize}
  \item Text section: the program code
  \item Data section: contains global variables (initialized and uninitialized)
  \item Heap: memory dynamically allocated during runtime
  \item Stack: contains temporarily variables, such as function parameters, return addresses, local variables
\end{itemize}

\image{images/Process.png}{4cm}{Memory layout for a process}

A process during execution cycles through the following states:

\begin{itemize}
  \item new: the process is created
  \item ready: The process is in a queue and is waiting to be assigned to a processor
  \item running: Instructions are being executed
  \item waiting: The process is in a queue and is waiting for some event to occur (ex. a memory transfer, an I/O)
  \item terminated: The process has finished execution
\end{itemize}

\image{images/Process FSM.png}{11cm}{Process state machine}

The OS also keeps track of the state of the process is stored in the RAM in a data structure called process control block (PCB). The PCB contains the following information:

\begin{itemize}
  \item Process state: running, waiting, etc.
  \item Program counter: location of next instruction
  \item CPU registers: contents of registers used by the process
  \item CPU scheduling information: priorities, scheduling queue pointers
  \item Memory-management information: memory allocated to the process
  \item Accounting/Debug information: CPU used, clock time elapsed since start, time limits
  \item I/O status information: I/O devices allocated to process, list of open files
  In Linux the PCB for every process is stored as a file in the /proc folder: \code{less /proc/<pid::self>/status}.
\end{itemize}

A \bold{context switch} is the process of storing the state of a process in the PCB, so that it can be restored and resume execution at a later point. Context switches can be categorized in:

\begin{itemize}
  \item voluntary context switch: the process stops itself because needs to wait for a resource
  \item nonvoluntary context switch: the processor decides to switch process
\end{itemize}

\section{Scheduling}
The CPU has a process scheduler, which decides which process to execute. The scheduler stores the processes in various queues:

\begin{itemize}
  \item Job queue: set of all processes in the system
  \item Ready queue: set of all processes residing in main memory, ready and waiting to execute
  \item Device queues: set of processes waiting for an I/O device
\end{itemize}

\image{images/Queues.png}{11cm}{Process queues}

\section{Process creation}
Processes are arranged in a tree structure: process can create other \italics{child} processes, which in turn can have other children. In Linux the process tree can be printed using \code{pstree}.

Resources among the parent and children can be shared in different ways:
\begin{itemize}
  \item Parent and children share all resources
  \item Children share subset of parent's resources
  \item Parent and child share no resources
  Moreover they have different options for execution:
  \item Parent and children execute concurrently
  \item Parent waits until children terminate
\end{itemize}
In a Linux system the root process that spawns all other processes is called \code{systemd}.

In UNIX processes are managed using the following system calls:
\begin{itemize}
  \item \code{fork()}: creates a new process by copying all data structures
  \item \code{clone()}: creates a new process which uses the same data structures as the parent process
  \item \code{exec()}: replaces the parent's memory with the children's one (machine code, data, heap, and stack)
  \item \code{wait()}: called by parent to wait for the end of the child's execution
\end{itemize}

\image{images/Process children.png}{14cm}{Creation of children processes}

\snippet{code/fork.c}{c}{A process that spawns a child process and waits for its termination}

\section{Communication between processes}
Processes can communicate using:
\begin{itemize}
  \item shared memory: processes that wish to communicate create a shared area of memory, that is managed directly by the processes
  \item message passing
\end{itemize}

\image{images/Process communication.png}{10cm}{Models of communication between processes}

\subsection{Shared memory}
Processes can communicate using shared memory. Processes can allocate a part in RAM as shared memory. Then they can access it by mapping it to their address space\footnote{array of addresses that the process is allowed to use}. In UNIX memory mapping is done by the \code{mmap()} system call.

\snippet{code/posix-producer.c}{c}{A process that creates a shared memory area and writes to it}
\snippet{code/posix-consumer.c}{c}{A process that opens a shared memory area and reads from it}

\subsection{Message passing}
Alternatively processes can communicate using message passing. This can be implemented in the following ways:
\begin{itemize}
  \item Shared memory (already seen in the previous section)
  \item Shared hardware bus
  \item Network
\end{itemize}
We can distinguish the channel on a logical level in the following ways:
\begin{itemize}
  \item Direct or indirect
  \item Synchronous or asynchronous
  \item Automatic or explicit buffering
\end{itemize}

\subsubsection{Pipes}
Pipes provide a way for to processes to communicate directly with each other. Pipes are accessed using the file descriptor. We can distinguish among two different types of pipes: ordinary pipes and named pipes.

Ordinary pipes cannot be accessed from outside the process that created it. Typically, a parent process creates a pipe and uses it to communicate with a child process that it created. Ordinary pipes are unidirectional, meaning that the parent process can only write to it and the child process can only read from it.

\snippet{code/pipe.c}{c}{A process that spawn a child and communicates to it using an ordinary pipe}

Named pipes can be accessed outside a parent-child relationship. They are bidirectional and multiple processes can read and write to it. When no process holds a reference to the file descriptor the pipe is destroyed by the system.

\snippet{code/named_pipe_write.c}{c}{A process that creates a named pipe and writes into it the content from the standard input}
\snippet{code/named_pipe_read.c}{c}{A process that reads from pipe and writes its content to the standard input}

\section{Threads}
A process can execute multiple instructions at once by using multiple threads. The OS keeps track of threads in the PCB. Threads share the same data and text (code) section and OS resources of the parent process. On the contrary each thread has its own registers, stack and program counter.

\image{images/Thread vs process.png}{13cm}{Single threaded process (left) and multithreaded process (right)}

\subsection{Difference between child processes and threads}
When a process is forked all information is duplicated (data, code, files), while threads of the same process share the same information. When a thread is created, the parent process has to specify which part of the code the thread has to execute. Therefore threads are more lightweight: resource sharing is easier than memory/message passing, they are quicker to create and can scale easily (can take advantage of multi-core architectures).

\subsection{Applications of threads}
Threads are used in client-server programs such as web server. Every time a request is received, the program creates a thread which fulfills the request. Another example is running background tasks, such as the spellchecker in a Word program.

\subsection{Linux threads}
Threads are created using the \code{clone()} call. Flags can be passed to the call to control the behavior of the calls.

The Linux scheduler doesn't make distinctions between threads and processes, but sees all of them as schedulable tasks.

\subsection{Multi-core programming}
The advantage of having multi-core systems is that tasks can be parallelized. There are two types of parallelism:
\begin{itemize}
  \item Data parallelism: data is distributed across multiple cores, each thread performs same operation
  \item Task parallelism: tasks are distributed across multiple cores, each thread performs a different operation
\end{itemize}

\subsection{Amdahl's Law}
Amdahl's Law describes the theoretical performance gain by using parallel code.
$$ \text{speedup} \le \left (S+\frac{1-S}{N} \right )^{-1} $$
Where S is the percentage of serial code, N is the number of cores, and $ \text{speedup} = 1 $ means that there is no speedup.

\subsection{Kernel threads}
User threads need to be mapped to kernel threads to be executed. A kernel thread is a kernel entity (an entity that can be handled by the system scheduler), like processes and interrupt handlers. User threads can be mapped to kernel threads in three ways: one-to-one (preferred by Linux and Windows), many-to-one, many-to-many.